{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Model: Used Car Price Prediction\n",
        "\n",
        "**Author:** Vinicius Moreira\n",
        "\n",
        "### Main Objective\n",
        "The objective of this project is to develop a high-performance regression model to predict the selling price of used cars in the Brazilian market. To do so, we will use a large dataset of real advertisements and apply advanced feature engineering techniques.\n",
        "\n",
        "### Technical Focus\n",
        "This notebook documents the complete process, focusing on:\n",
        "* **Statistical and Exploratory Data Analysis (EDA):** Deeply understand the characteristics and distributions of the dataset.\n",
        "* **Feature Engineering:** Create new informative variables from the raw data.\n",
        "* **Model Construction and Fine-Tuning:** Adapt a pre-trained Transformer model (`BERTimbau`) for the tabular regression task, building the training loop \"by hand\" with PyTorch.\n",
        "* **GPU Acceleration:** Utilize a local GPU (NVIDIA CUDA) to massively accelerate model training time."
      ],
      "metadata": {
        "id": "zbyodSxkPLm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EciR75wF0So"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import joblib\n",
        "import json\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tqdm.auto import tqdm\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams.update({'font.size': 12, 'axes.titlesize': 14, 'axes.labelsize': 12})\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"GPU not detected, training will be done on CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading the Data\n",
        "\n",
        "The foundation of any Machine Learning project is data. In this section, i will establish a connection to Google Drive, where our dataset is persistently stored, and load it into a Pandas dataframe. This will allow to begin manipulation and analysis"
      ],
      "metadata": {
        "id": "FjDt6b2LQc9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_do_arquivo = os.path.join('data', 'raw', 'car_prices.csv')\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(caminho_do_arquivo)\n",
        "    print(f\"dataset loaded, finded {df.shape[0]} lines and {df.shape[1]} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"docs not finded in '{caminho_do_arquivo}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"unexpected error: {e}\")"
      ],
      "metadata": {
        "id": "U8nKoPL9Qp81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Initial Inspection and Data Quality Diagnosis\n",
        "\n",
        "Before any in-depth analysis, it is crucial to perform a health diagnosis of my data. In this step, i will answer three key questions:\n",
        "1. What is the structure of the data?\n",
        "2. What are the data types of each column?\n",
        "3. How big is the missing data problem?\n",
        "\n",
        "My hypothesis is that columns that rely on manual data entry, such as trim or interior, will have a significant percentage of null values, requiring a specific treatment strategy."
      ],
      "metadata": {
        "id": "Wsuavg1PS4n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5 firs lines\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\ngeral information and type of data\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n% null values per column\")\n",
        "porcentagem_nulos = (df.isnull().sum() / len(df)) * 100\n",
        "print(porcentagem_nulos[porcentagem_nulos > 0].sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "DcRxyF64TZFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Cleaning and Missing Values ​​Handling\n",
        "\n",
        "Based on the previous diagnosis, i will now clean and prepare my dataset. The strategy i will be:\n",
        "1. Remove rows that contain null values ​​in columns that are almost complete.\n",
        "2. Fill the null values ​​of the most important categorical columns with the mode (the most frequent value).\n",
        "3. Fill the null values ​​of the numeric column condition with the median.\n",
        "4. Convert the saledate column to datetime format."
      ],
      "metadata": {
        "id": "3AMMn-lMdKsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_limpo = df.copy()\n",
        "\n",
        "colunas_quase_completas = ['sellingprice', 'mmr', 'odometer', 'color', 'interior', 'vin', 'saledate']\n",
        "linhas_antes = len(df_limpo)\n",
        "df_limpo.dropna(subset=colunas_quase_completas, inplace=True)\n",
        "linhas_depois = len(df_limpo)\n",
        "print(f\"removed {linhas_antes - linhas_depois} lines on light cleaning\")\n",
        "\n",
        "colunas_para_imputar_moda = ['make', 'model', 'trim', 'body', 'transmission']\n",
        "for col in colunas_para_imputar_moda:\n",
        "    moda = df_limpo[col].mode()[0]\n",
        "    df_limpo[col] = df_limpo[col].fillna(moda)\n",
        "    print(f\"null values in '{col}' filled with: '{moda}'\")\n",
        "\n",
        "mediana_condition = df_limpo['condition'].median()\n",
        "df_limpo['condition'].fillna(mediana_condition, inplace=True)\n",
        "print(f\"null values ​​in 'condition' filled with median: {mediana_condition}\")\n",
        "\n",
        "\n",
        "df_limpo['saledate'] = pd.to_datetime(df_limpo['saledate'], utc=True, errors='coerce')\n",
        "df_limpo.dropna(subset=['saledate'], inplace=True)\n",
        "print(\"column 'saledate' converted to datetime type\")\n",
        "\n",
        "valores_nulos_restantes = df_limpo.isnull().sum().sum()\n",
        "if valores_nulos_restantes == 0:\n",
        "    print(\"There are no more null values ​​in the dataset\")\n",
        "else:\n",
        "    print(f\"there are still some left {valores_nulos_restantes} null values\")\n",
        "\n",
        "print(f\"\\nfinal DataFrame size: {df_limpo.shape[0]} lines\")"
      ],
      "metadata": {
        "id": "oZS1Z08VdXMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature engineering (new columns)\n",
        "\n",
        "With the data cleaned, i can now create more informative features for our model. Machine learning models rarely learn well from raw data; i need features that capture patterns more explicitly.\n",
        "\n",
        "i will create two very important features:\n",
        "1. **Car Age:** Instead of model year, car age is a much more powerful feature for predicting depreciation and price.\n",
        "2. **Sale Date Features:** We will extract the month, day of the week, and day of the year from the sale date. This can capture market seasonality (e.g. do cars sell more in December?)."
      ],
      "metadata": {
        "id": "-THV6_0Qg0pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_features = df_limpo.copy()\n",
        "\n",
        "ano_atual = pd.to_datetime('now').year\n",
        "df_features['age'] = ano_atual - df_features['year']\n",
        "print(\"feature 'age' created\")\n",
        "\n",
        "df_features['sale_month'] = df_features['saledate'].dt.month\n",
        "df_features['sale_dayofweek'] = df_features['saledate'].dt.dayofweek\n",
        "df_features['sale_dayofyear'] = df_features['saledate'].dt.dayofyear\n",
        "print(\"features of data ('sale_month', 'sale_dayofweek', 'sale_dayofyear') created\")\n",
        "\n",
        "df_features = df_features.drop(['year', 'saledate'], axis=1)\n",
        "print(\"original columns 'year' e 'saledate' removed\")\n",
        "\n",
        "print(\"\\nsample of dataframe with new features\")\n",
        "display(df_features[['age', 'sale_month', 'sale_dayofweek', 'sale_dayofyear', 'sellingprice']].head())"
      ],
      "metadata": {
        "id": "iVWldJH3hRoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Advanced Feature Engineering\n",
        "\n",
        "To further enrich the dataset, I will create interaction and popularity features. These features can capture more complex relationships that are not obvious to the model if it just looks at the columns individually.\n",
        "\n",
        "1. **Popularity Features:** I will calculate the frequency of each make and model to create features that represent how common or rare a vehicle is in the market.\n",
        "\n",
        "2. **Interaction Feature (Vehicle Usage):** I will create a km_per_year feature by dividing the odometer by the age of the car. This normalizes the mileage by age, giving a much clearer measure of the \"usage level\" of the vehicle."
      ],
      "metadata": {
        "id": "0KHiugIY1SYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_advanced_features = df_features.copy()\n",
        "\n",
        "make_counts = df_advanced_features['make'].value_counts(normalize=True)\n",
        "df_advanced_features['make_popularity'] = df_advanced_features['make'].map(make_counts)\n",
        "print(\"feature 'make_popularity' created\")\n",
        "\n",
        "model_counts = df_advanced_features['model'].value_counts(normalize=True)\n",
        "df_advanced_features['model_popularity'] = df_advanced_features['model'].map(model_counts)\n",
        "print(\"feature 'model_popularity' created\")\n",
        "\n",
        "df_advanced_features['km_per_year'] = df_advanced_features['odometer'] / (df_advanced_features['age'] + 1)\n",
        "print(\"feature 'km_per_year' created\")\n",
        "\n",
        "print(\"\\nsample of dataframe with new features\")\n",
        "display(df_advanced_features[['make', 'make_popularity', 'model', 'model_popularity', 'age', 'odometer', 'km_per_year', 'sellingprice']].head())\n",
        "\n",
        "df_features = df_advanced_features\n",
        "print(\"\\nmain DataFrame updated with new features\")"
      ],
      "metadata": {
        "id": "E5B8MRRe1sii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preparing the Transformer: Converting Rows to Strings\n",
        "\n",
        "Transformer models, such as BERT, which I will use for fine-tuning, are experts at understanding the meaning and relationships in sequences of text. In order for it to \"understand\" the data about a car, we need a strategy to convert each row of our dataframe into a single \"sentence\" that describes the vehicle in a structured way.\n",
        "\n",
        "**My approach will be:**\n",
        "1. Select the most relevant features that we want the model to consider.\n",
        "2. For each car (row), create a single string that combines the feature name and its value, separating them with a special character.\n",
        "\n",
        "For example, a car will become the sentence:\n",
        "`make [Ford] | model [Ka] | age [7] | odometer [50000] | [manual] transmission\n",
        "\n",
        "This textual representation allows the Transformer's \"attention\" mechanism to learn the complex interactions between the different features of the car, much like it learns the relationships between words in a typical sentence."
      ],
      "metadata": {
        "id": "ldk_UO61kyOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features_para_texto = [\n",
        "    'make', 'model', 'trim', 'body', 'transmission', 'color',\n",
        "    'interior', 'age', 'odometer', 'sale_month', 'sale_dayofweek'\n",
        "]\n",
        "\n",
        "def criar_representacao_textual(row):\n",
        "    partes = [f\"{coluna} [{str(row[coluna])}]\" for coluna in features_para_texto]\n",
        "    return \" | \".join(partes)\n",
        "\n",
        "df_features['text'] = df_features.apply(criar_representacao_textual, axis=1)\n",
        "print(\"column text created\")\n",
        "\n",
        "df_final = df_features[['text', 'sellingprice']].copy()\n",
        "\n",
        "print(\"\\nsample of strings created\")\n",
        "for i in range(5):\n",
        "    print(f\"Carro {i}:\")\n",
        "    print(df_final['text'].iloc[i])\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "Q1Zqcb7Xk8R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Splitting the Data: Creating the Training and Test Sets\n",
        "\n",
        "One of the most important rules in Machine Learning is to never evaluate the model with the same data it used to learn.\n",
        "\n",
        "To ensure an honest assessment of the models performance, I will split the dataset into two distinct groups using scikit-learn's train_test_split function:\n",
        "\n",
        "1. **Training Set (80% of the data):** This will be used during the training phase so that the model learns the patterns between a car's features and its price.\n",
        "\n",
        "2. **Test Set (remaining 20%):** This will be \"saved\" and untouched. We will only use this set at the end of the project to see how well the model generalizes to data it has never seen before.\n",
        "\n",
        "In addition, I will apply the logarithmic transformation (np.log1p) to the target variable (sellingprice) before the split. As we saw in the exploratory analysis, this helps to normalize the price distribution, which stabilizes and improves the training of the neural network."
      ],
      "metadata": {
        "id": "JwDx98oYmaj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_final['text']\n",
        "y = df_final['sellingprice']\n",
        "\n",
        "y_log = np.log1p(y)\n",
        "print(\"target var 'sellingprice' transformed with log(1+x).\")\n",
        "\n",
        "X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
        "    X, y_log, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\ndivision conclude\")\n",
        "print(f\"training set size (X_train): {X_train.shape[0]} samples\")\n",
        "print(f\"test set size (X_test):   {X_test.shape[0]} samples\")\n",
        "print(f\"training target size (y_train_log): {y_train_log.shape[0]} samples\")\n",
        "print(f\"test target size (y_test_log):   {y_test_log.shape[0]} samples\")"
      ],
      "metadata": {
        "id": "WhuI4WrBorPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Tokenization: Translating Text into Model Language\n",
        "\n",
        "Neural networks, and especially Transformer models, don’t understand words like ‘Ford’ or ‘auto’. They understand tokens. Tokenization is the process of “translating” the descriptive phrases I created for each car into a sequence of numbers (tokens) that the BERT model (BERTimbau) has been pre-trained to recognize.\n",
        "\n",
        "For each “phrase”, the tokenizer will generate two main outputs:\n",
        "* **input_ids:** The list of numeric IDs that represent each word or subword. Each number corresponds to an entry in the model’s vocabulary.\n",
        "* **attention_mask:** A list of 1s and 0s that tells the model which tokens are actual words (1) and which are “padding” (0) added just to make sure all the sequences in a batch are the same length.\n",
        "\n",
        "I will use Hugging Face's transformers library to load the exact tokenizer corresponding to the BERTimbau model, ensuring full compatibility."
      ],
      "metadata": {
        "id": "f8qvMJNNqF0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "\n",
        "print(f\"loading tokenizer of model: '{MODEL_NAME}'...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "print(\"tokenizer loaded\")\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "print(\"\\ntokenizing data's training\")\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LEN,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(\"tokenizing test data's\")\n",
        "test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LEN,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(\"\\ntokenizing loaded\")\n",
        "print(f\"shape of the input_ids of training: {train_encodings['input_ids'].shape}\")"
      ],
      "metadata": {
        "id": "duj-cmbIqiT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Creating the PyTorch Dataset\n",
        "\n",
        "Now that the data is in the correct numeric format (tokens), I need to organize it into a structure that PyTorch understands natively. To do this, I'll create a custom Dataset class.\n",
        "\n",
        "To create a custom Dataset in PyTorch, the class needs three special methods:\n",
        "* `__init__()`: The constructor, where we receive and store our data.\n",
        "* `__len__()`: A simple method that returns the total number of samples in the dataset.\n",
        "* `__getitem__()`: The most important method. This is the one that is called when we request dataset[52]. It is responsible for fetching and returning a single sample of data."
      ],
      "metadata": {
        "id": "FotQvzBLwSy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CarPriceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = CarPriceDataset(train_encodings, y_train_log.values)\n",
        "test_dataset = CarPriceDataset(test_encodings, y_test_log.values)\n",
        "\n",
        "print(\"training and test dataset created\")\n",
        "print(f\"number of samples in train_dataset: {len(train_dataset)}\")\n",
        "print(f\"number of samples in test_dataset: {len(test_dataset)}\")\n",
        "\n",
        "print(\"\\none example item of dataset :\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "dF6_dhz-w3CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Creating the DataLoader for Batch Training\n",
        "\n",
        "Its function is to take the data from the Dataset and group it into batches. Instead of training the model with one car at a time, which would be very inefficient, I will train it with a batch of, for example, 16 or 32 cars simultaneously. This maximizes the use of the GPU, since it is specialized in doing many mathematical operations in parallel.\n",
        "\n",
        "In addition, the training DataLoader has a crucial function: to shuffle the data at each new \"epoch\" (a complete pass through the training dataset). This ensures that the model does not learn any accidental order in the data, which improves its generalization ability."
      ],
      "metadata": {
        "id": "xfqKTAUyyFGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"dataloarder of training and tests created\")\n",
        "print(f\"the training DataLoader will have approximately {len(train_loader)} lots per epoch\")\n",
        "print(f\"the test DataLoader will be approximately {len(test_loader)} lots\")"
      ],
      "metadata": {
        "id": "HiCgz-JaynCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Building the Model Architecture with PyTorch and Transformers\n",
        "\n",
        "This is the core AI engineering step of the project: defining the \"brain\" of the system, which will learn to predict prices. To do this, I will use a powerful and efficient technique called Transfer Learning.\n",
        "\n",
        "We will not build a neural network from scratch. Instead, we will take a giant pre-trained model, BERTimbau, and fine-tune it for the specific regression task.\n",
        "\n",
        "Our architecture will have two main parts:\n",
        "\n",
        "1. **The Backbone:** The pre-trained BERTimbau. Its job will be to read the \"sentence\" that describes the car (e.g. `make [Ford] | model [Ka]...`) and generate a rich vector representation that captures all the meaning and relationships between the features.\n",
        "\n",
        "2. **The Regression \"Head\":** This is the custom part. We will add on top of BERT a small neural network with a few linear layers. Its job is to take the rich BERT representation and map it to a single number: the predicted price of the car.\n",
        "\n",
        "To combat overfitting, I’ll add Dropout layers to the regression “head.” During training, they randomly turn off some neurons, forcing the network to learn more robustly and generalize better to new data."
      ],
      "metadata": {
        "id": "Gv1y3rBr0jxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionTransformer(nn.Module):\n",
        "    def __init__(self, model_name=MODEL_NAME):\n",
        "        super(RegressionTransformer, self).__init__()\n",
        "\n",
        "        print(f\"loading model: {model_name}...\")\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(self.bert.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        print(\"model defined\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs.pooler_output\n",
        "        price_prediction = self.regressor(pooled_output)\n",
        "        return price_prediction\n",
        "\n",
        "model = RegressionTransformer()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"\\nmodel moved to device: {device}\")"
      ],
      "metadata": {
        "id": "gNtw8qTC1clv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Defining the Loss Function and the Optimizer\n",
        "\n",
        "Now that we have the model architecture, we need two last tools for training:\n",
        "\n",
        "**1. The Loss Function The Error \"Ruler\"**\n",
        "Think of the Loss Function as a ruler. For each price prediction the model makes, we use this ruler to measure the \"distance\" (the error) between the prediction and the actual price of the car. The goal of training is to make the value of this \"ruler\" as small as possible.\n",
        "\n",
        "For the regression problem, I will use L1Loss, also known as Mean Absolute Error (MAE). It simply calculates the average absolute difference between the actual and predicted value (`|actual - predicted|`). It is robust and easy to interpret: an MAE of 0.1, for example, will mean that the model is missing, on average, R$ 0.1 in the log value of the price.\n",
        "\n",
        "**2. The Optimizer**\n",
        "It looks at the error measured by the ruler and, using the gradient calculated by backpropagation, adjusts each of the millions of weights in the neural network in the right direction to reduce that error.\n",
        "\n",
        "I will use AdamW, one of the most popular and effective optimizers for training Transformers. It is an evolution of Adam that incorporates a technique called Weight Decay, one of the weapons against overfitting that helps keep the model weights small and prevent the model from becoming too complex.\n",
        "\n",
        "In addition, I will use a Learning Rate Scheduler, which adjusts the intensity of learning over time, starting faster and making finer adjustments at the end."
      ],
      "metadata": {
        "id": "oCfwx82y3y_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 12\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "loss_function = nn.L1Loss()\n",
        "print(\"MAE defined\")\n",
        "\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "print(f\"optimizer AdamW defined with learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "print(\"Learning Rate Scheduler configured\")"
      ],
      "metadata": {
        "id": "uu0UI1-x69Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. The Training and Evaluation Functions\n",
        "\n",
        "With all of our components defined (model, dataloaders, optimizer, etc.), let's now create the logic that ties them together. To keep the code organized, I'll create two separate functions:\n",
        "\n",
        "1. **train_epoch() (The Training Function):**\n",
        "This function will be responsible for running a full epoch of training. An \"epoch\" means going through our entire train_loader once. For each batch of data, this function will:\n",
        "* Move the data to the GPU.\n",
        "* Make a prediction with the model (forward pass).\n",
        "* Calculate the error (loss) using our L1Loss.\n",
        "* Calculate the gradients with backpropagation to know how to correct each model weight.\n",
        "* Update the model weights using the optimizer.\n",
        "* Adjust the learning rate with the scheduler.\n",
        "* At the end, it will return the average error for that epoch.\n",
        "\n",
        "2. **evaluate() (The Evaluation Function):**\n",
        "This function is simpler. It will go through the test_loader and make predictions, but without updating the weights (no backpropagation or optimizer). Its only goal is to calculate the average error on the test set. It is with this function that I will monitor whether the model is actually learning to generalize and detect overfitting (when the training error drops, but the test error starts to rise)."
      ],
      "metadata": {
        "id": "0Tv-4Suw9JlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Treinando...\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        loss = loss_fn(outputs.squeeze(), labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, loss_fn, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Avaliando...\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            loss = loss_fn(outputs.squeeze(), labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "print(\"funcs train_epoch and evaluate defined's\")"
      ],
      "metadata": {
        "id": "lEU7ft-F91I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. The Training Loop\n",
        "\n",
        "This is the final cell of the modeling process. Here, I orchestrate everything I have built.\n",
        "\n",
        "The following loop will loop through the data for the number of EPOCHS I have defined. At each epoch, it will do the following:\n",
        "\n",
        "1. Call the train_epoch function to train the model on all the training data and adjust its weights.\n",
        "\n",
        "2. Call the evaluate function to evaluate the performance of the trained model on the test set, which it has never seen before.\n",
        "\n",
        "3. Print the training and validation loss so we can track the learning.\n",
        "\n",
        "4. Check if the validation error for the current epoch is the lowest we have ever seen. If so, it saves the model at that point. This ensures that at the end, I will have saved the best possible \"brain\".\n",
        "\n",
        "At the end, I will plot a graph to visualize the learning curve of the model."
      ],
      "metadata": {
        "id": "6wQhKUa9_BA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = defaultdict(list)\n",
        "best_loss = float('inf')\n",
        "\n",
        "print(\"start fine-tuning\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    print(f\"\\nepoch {epoch + 1}/{EPOCHS} ---\")\n",
        "\n",
        "    train_loss = train_epoch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler\n",
        "    )\n",
        "\n",
        "    val_loss = evaluate(\n",
        "        model,\n",
        "        test_loader,\n",
        "        loss_function,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    print(f\"end of epoch {epoch + 1} | time: {epoch_time:.2f}s\")\n",
        "    print(f\"training loss: {train_loss:.4f} | validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        print(\"saving model\")\n",
        "        caminho_para_salvar = os.path.join('data', 'raw', 'best_model_state.pth')\n",
        "        torch.save(model.state_dict(), caminho_para_salvar)\n",
        "        best_loss = val_loss\n",
        "\n",
        "print(\"\\nfine-tuning conclude\")\n",
        "print(f\"best validation loss achieved: {best_loss:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['train_loss'], label='Loss de Treino')\n",
        "plt.plot(history['val_loss'], label='Loss de Validação')\n",
        "plt.title('loss history by season')\n",
        "plt.ylabel('Loss (MAE Log-transformed)')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LHrAABY8_gGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code uses the 'history' and 'best_loss' variables that were saved during fine-tuning.\n",
        "\n",
        "print(f\"best validation loss achieved: {best_loss:.4f}\")\n",
        "print(f\"-----------------------------------------------\")\n",
        "\n",
        "epochs_completed = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(epochs_completed, history['train_loss'], 'b-o', label='training loss')\n",
        "plt.plot(epochs_completed, history['val_loss'], 'r-o', label='validation loss')\n",
        "\n",
        "best_epoch = np.argmin(history['val_loss']) + 1\n",
        "plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'best epoch ({best_epoch}) a {best_loss:.4f}')\n",
        "\n",
        "plt.title('soss History by epoch')\n",
        "plt.ylabel('loss (MAE transformed-log)')\n",
        "plt.xlabel('epoch')\n",
        "plt.xticks(list(epochs_completed))\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dra_5cIsS-p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. Model Evaluation and Inference on Samples\n",
        "\n",
        "With the model trained, we come to the final step: evaluating its performance qualitatively and quantitatively.\n",
        "\n",
        "In this section, I will do the following:\n",
        "1. **Load the Best Model:** I will load the weights from the best_model_state.pth file that I saved during training. This ensures that I am using the version of the model with the lowest validation error.\n",
        "2. **Make Predictions:** I will take some samples from the test set (data that the model has never seen) and run them through the model to get its price constraints.\n",
        "3. **Interpret the Results:** The output of the model is a log-transformed price. I need to reverse this transformation (np.expm1) to see the predicted price in Reais and compare it to the actual price. 4. **Calculate the Mean Absolute Error (MAE):** You calculate the average error of the variations across the entire test set to have a final, concrete performance metric: \"On average, the model misses the price of cars by R$X.\""
      ],
      "metadata": {
        "id": "J8ArRymxsh0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = os.path.join('data', 'raw', 'best_model_state.pth')\n",
        "inference_model = RegressionTransformer(MODEL_NAME)\n",
        "inference_model.load_state_dict(torch.load(save_path))\n",
        "inference_model.to(device)\n",
        "inference_model.eval()\n",
        "print(\"model loaded\")\n",
        "\n",
        "print(\"\\ntesting model in random samples\")\n",
        "\n",
        "amostras_teste = X_test.sample(5, random_state=42)\n",
        "indices = amostras_teste.index\n",
        "\n",
        "for idx in indices:\n",
        "    texto = X_test.loc[idx]\n",
        "    preco_real_log = y_test_log.loc[idx]\n",
        "\n",
        "    encoded_text = tokenizer.encode_plus(\n",
        "        texto,\n",
        "        max_length=MAX_LEN,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_text['input_ids'].to(device)\n",
        "    attention_mask = encoded_text['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        preco_predito_log = inference_model(input_ids, attention_mask)\n",
        "\n",
        "    preco_real = np.expm1(preco_real_log)\n",
        "    preco_predito = np.expm1(preco_predito_log.cpu().numpy()[0][0])\n",
        "\n",
        "    erro = abs(preco_real - preco_predito)\n",
        "\n",
        "    print(f\"\\nCarro: {texto[:80]}...\")\n",
        "    print(f\"  -> Preço Real: R$ {preco_real:,.2f}\")\n",
        "    print(f\"  -> Preço Previsto: R$ {preco_predito:,.2f}\")\n",
        "    print(f\"  -> Erro: R$ {erro:,.2f}\")\n",
        "\n",
        "print(\"\\ncalculing MAE\")\n",
        "y_pred_log_list = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"making batch predictions\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = inference_model(input_ids, attention_mask)\n",
        "        y_pred_log_list.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "y_test_real = np.expm1(y_test_log.values)\n",
        "y_pred_real = np.expm1(y_pred_log_list)\n",
        "\n",
        "mae = mean_absolute_error(y_test_real, y_pred_real)\n",
        "\n",
        "print(f\"\\nMean Absolute Error of the model on the test set is: R$ {mae:,.2f}\")"
      ],
      "metadata": {
        "id": "HnRzAyfQsiJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. Saving the Final Artifacts for Production\n",
        "\n",
        "In addition to the model weights, I need to save the preprocessing pipeline (ColumnTransformer) and the list of features that the model expects. This ensures that the future API can replicate exactly the same steps I took in training. I will use the joblib library to save the scikit-learn object."
      ],
      "metadata": {
        "id": "l5RuZh__wAD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']\n",
        "numeric_features = ['age', 'odometer', 'sale_month', 'sale_dayofweek', 'sale_dayofyear', 'make_popularity', 'model_popularity', 'km_per_year']\n",
        "text_feature = 'text'\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "features_para_texto = numeric_features + categorical_features\n",
        "\n",
        "def criar_representacao_textual(row):\n",
        "    partes = [f\"{coluna}[{str(row[coluna])}]\" for coluna in features_para_texto]\n",
        "    return \" | \".join(partes)\n",
        "\n",
        "X_text = df_features.apply(criar_representacao_textual, axis=1)\n",
        "y_log = np.log1p(df_features['sellingprice'])\n",
        "\n",
        "save_path_model = os.path.join('data', 'raw', 'best_model_state.pth')\n",
        "torch.save(model.state_dict(), save_path_model)\n",
        "\n",
        "save_path_preprocessor = os.path.join('data', 'raw', 'preprocessor.pkl')\n",
        "joblib.dump(preprocessor, save_path_preprocessor)\n",
        "\n",
        "save_path_features = os.path.join('data', 'raw', 'model_features.json')\n",
        "features_dict = {'numeric': numeric_features, 'categorical': categorical_features}\n",
        "with open(save_path_features, 'w') as f:\n",
        "    json.dump(features_dict, f)\n",
        "\n",
        "print(f\"model saved in: {save_path_model}\")\n",
        "print(f\"pre-processor saved in: {save_path_preprocessor}\")\n",
        "print(f\"model features saved in: {save_path_features}\")"
      ],
      "metadata": {
        "id": "NLmBUGjnwXbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
